---
title: "CensSpBayesVignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{CensSpBayesVignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Demonstration for the Baseline Model with Gaussian prior

```{r setup}
library(CensSpBayes)
```

We use the dataset \texttt{simdat} to provide a demonstration on how to use the R-package \texttt{CensSpBayes} and its functions.

```{r,echo=TRUE}
##  Loading the synthetic data
data(simdat)

## Description of the data
?simdat

## Variables present in the data
names(simdat)
```

Since this data is simulated, all observations are available and nothing is censored or missing. First, we shall proceed to hold off about $20\%$ of the data as prediction location and data. Then we shall use the variable \texttt{cutoff.Y} to create a censored version of the response.

```{r,echo=TRUE}
permind = sample(1:nrow(simdat),nrow(simdat))
obsind = permind[1:8000]
predind = permind[8001:10000]

Y.obstrue = simdat$Y[obsind]
Y.predtrue = simdat$Y[predind]
S.obs = cbind(simdat$x[obsind],simdat$y[obsind])
S.pred = cbind(simdat$x[predind],simdat$y[predind])
cutoff.Y.obs = simdat$cutoff.Y[obsind]

Y.censind = which(Y.obstrue < cutoff.Y.obs)
Y.obs = Y.obstrue
Y.obs[Y.censind] = cutoff.Y.obs[Y.censind]
```

We shall now use this data, namely \texttt{Y.obs}, \texttt{S.obs}, \texttt{cutoff.Y.obs} and \texttt{S.pred} to run \texttt{CensSpBayes} function.

But first, we require the mass and stiffness matrices for the SPDE mesh and index matrices for the observation and prediction locations. We do this by using the function \texttt{create_inla_mats}.

```{r,echo=TRUE}
inla.mats = create_inla_mats(S=S.obs,S_pred=S.pred,
                             offset=c(0.01, 0.2),
                             cutoff=0.05,
                             max.edge=c(0.01, 0.1))
```

The inputs \texttt{offset}, \texttt{cutoff}, \texttt{max.edge} determines the mesh that will be used to approximate the Gaussian process and can have effects on how well the method performs in approximating the underlying Gaussian process.

Once we have these matrices, we are ready to fit our Bayesian MCMC model to the censored data. Since we have no covariates, we create our design matrix using an intercept term and the locations themselves.

```{r,echo=TRUE}
X.obs = matrix(1,nrow(S.obs),1)
X.pred = matrix(1,nrow(S.pred),1)

set.seed(100)
out = CensSpBayes(Y=Y.obs,S= S.obs,X=X.obs,
                  cutoff_Y=cutoff.Y.obs,
                  S_pred=S.pred,X_pred=X.pred,
                  inla_mats=inla.mats,
                  rho_init=0.1, rho_upper=0.25,
                  iters=200,burn=100,thin=2)
```


You can get the samples and the traceplots by accessing the output as you would for any other list output. 

```{r,echo=TRUE}
str(out)

names(out)

## Access elements

out$tau

## Get traceplots
plot(out$tau,type="l",xlab="iters",ylab="tau")

```

# Demonstration for variable Selection using \texttt{CensSpBayes}

You can use the function \texttt{CensSpBayesVS} to fit the CensSpBayes model and perform variable selection. This function allows for the choice of prior to be between Gaussian and Hosrseshoe+ (Bhadra et al., 2017) using the input \texttt{prior} which has two possible values of input \texttt{"gaussian"} or \texttt{"hsp"} for the two prior choices. There is also another input to this function named \texttt{vss} which allows values \texttt{"none"}, \texttt{"cr"}, \texttt{"2means"}, and \texttt{"hsp"} as demonstrated below: 

```{r,echo=TRUE,eval=FALSE}
out = CensSpBayesVS(Y=Y.obs,S= S.obs,X=X.obs,
                  cutoff_Y=cutoff.Y.obs,
                  S_pred=S.pred,X_pred=X.pred,
                  inla_mats=inla.mats,
                  rho_init=0.1, rho_upper=0.25,
                  prior="gaussian",
                  vss="none",
                  iters=200,burn=100,thin=2) ## Same as CensSpBayes

out = CensSpBayesVS(Y=Y.obs,S= S.obs,X=cbind(X.obs,S.obs),
                  cutoff_Y=cutoff.Y.obs,
                  S_pred=S.pred,X_pred=cbind(X.pred,S.pred),
                  inla_mats=inla.mats,
                  rho_init=0.1, rho_upper=0.25,
                  prior="gaussian",
                  vss="cr",
                  addpar=0.95,
                  iters=200,burn=100,thin=2) ## Gaussian prior but only coefficients whose 95% (addpar*100%) CI does not include 0 are included in the results. Can be used with prior="hsp"

out = CensSpBayesVS(Y=Y.obs,S= S.obs,X=cbind(X.obs,S.obs),
                  cutoff_Y=cutoff.Y.obs,
                  S_pred=S.pred,X_pred=cbind(X.pred,S.pred),
                  inla_mats=inla.mats,
                  rho_init=0.1, rho_upper=0.25,
                  prior="hsp",
                  vss="2means",
                  addpar=20,
                  iters=200,burn=100,thin=2) ## Horseshoe+ prior and only coefficients deemed significant by a two-means clustering (tuning parameter set at addpar) is included in the results. Can be used with prior="gaussian"

out = CensSpBayesVS(Y=Y.obs,S= S.obs,X=cbind(X.obs,S.obs),
                  cutoff_Y=cutoff.Y.obs,
                  S_pred=S.pred,X_pred=c(X.pred,S.pred),
                  inla_mats=inla.mats,
                  rho_init=0.1, rho_upper=0.25,
                  prior="hsp",
                  vss="hsp",
                  addpar=0.5,
                  iters=200,burn=100,thin=2) ## Horseshoe+ prior and only coefficients with posterior inclusion probability bigger than addpar is included in the results. Can NOT be used with prior="gaussian"
```

The addpar parameter has different meaning for different choices of \texttt{vss}. It gives the confidence coefficient for \texttt{vss="cr"}. It gives the posterior inclusion probability beyond which to include the covariates as significant for \texttt{vss="hsp"}. It gives the the tuning parameter value for two-means clustering required for the algorithm when \texttt{vss="2means"}. See Chapagain and Pati (2025) and Li and Pati (2017) for details.


# References

1. Bhadra, A., Datta, J., Polson, N.G. and Willard, B., 2017. The Horseshoe+ Estimator of Ultra-Sparse Signals. \textit{Bayesian Analysis}, 12(4), pp: 1105-1131.
1. Chapagain, N. and Pati, D., 2025. VsusP: Variable Selection using Shrinkage Priors [R package].
1. Li, H. and Pati, D., 2017. Variable selection using shrinkage priors. \textit{Computational Statistics \& Data Analysis}, 107, pp. 107-119.
